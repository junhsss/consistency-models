{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMuCZYN/WQPLvMBhitJgGv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junhsss/consistency-models/blob/main/examples/consistency_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Consistency Models** ðŸŒƒ\n",
        "*...using `consistency`*\n",
        "\n",
        "**Consistency Models** are a new family of generative models that achieve high sample quality without adversarial training. They support *fast one-step generation* by design, while still allowing for few-step sampling to trade compute for sample quality. It's amazing!"
      ],
      "metadata": {
        "id": "CGWq9y6LU692"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "Please make sure you are using a GPU runtime to run this notebook. If the following command fails, use the `Runtime` menu above and select `Change runtime type`."
      ],
      "metadata": {
        "id": "-OsNEVyTg_Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "LAtKPa8_hQjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets wandb consistency==0.2.2"
      ],
      "metadata": {
        "id": "IpmvA2RSUctd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "ldzQ_-1DYrIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"cifar10\"\n",
        "RESOLUTION = 32\n",
        "BATCH_SIZE = 300\n",
        "MAX_EPOCHS = 600\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "SAMPLES_PATH = \"./samples\"\n",
        "NUM_SAMPLES = 64\n",
        "SAMPLE_STEPS = 1  # Set this value larger if you want higher sample quality."
      ],
      "metadata": {
        "id": "CWeu8NA6WCni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_name: str, dataset_config_name=None):\n",
        "        self.dataset = load_dataset(\n",
        "            dataset_name,\n",
        "            dataset_config_name,\n",
        "            split=\"train\",\n",
        "        )\n",
        "        self.image_key = [\n",
        "            key for key in (\"image\", \"img\") if key in self.dataset[0]\n",
        "        ][0]\n",
        "        self.augmentations = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(\n",
        "            RESOLUTION,\n",
        "            interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "        ),\n",
        "        transforms.CenterCrop(RESOLUTION),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> torch.Tensor:\n",
        "        return self.augmentations(self.dataset[index][self.image_key].convert(\"RGB\"))\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    Dataset(DATASET_NAME),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "J_dZ-sFIVv7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Models\n",
        "\n",
        "`Consistency` accepts any unet-like model as its backbone. \n",
        "We recommend `UNet2DModel` of `diffusers` ðŸ¤— as a default option."
      ],
      "metadata": {
        "id": "4S6bE5ANaaur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "from consistency import Consistency\n",
        "from consistency.loss import PerceptualLoss\n",
        "\n",
        "consistency = Consistency(\n",
        "    model=UNet2DModel(\n",
        "        sample_size=RESOLUTION,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        layers_per_block=2,\n",
        "        block_out_channels=(128, 128, 256, 256, 512, 512),\n",
        "        down_block_types=(\n",
        "            \"DownBlock2D\",\n",
        "            \"DownBlock2D\",\n",
        "            \"DownBlock2D\",\n",
        "            \"DownBlock2D\",\n",
        "            \"AttnDownBlock2D\",\n",
        "            \"DownBlock2D\",\n",
        "        ),\n",
        "        up_block_types=(\n",
        "            \"UpBlock2D\",\n",
        "            \"AttnUpBlock2D\",\n",
        "            \"UpBlock2D\",\n",
        "            \"UpBlock2D\",\n",
        "            \"UpBlock2D\",\n",
        "            \"UpBlock2D\",\n",
        "        ),\n",
        "    ),\n",
        "    # You could use multiple net types. \n",
        "    # Recommended setting is \"squeeze\" + \"vgg\"\n",
        "    # loss_fn=PerceptualLoss(net_type=(\"squeeze\", \"vgg\"))\n",
        "    # See https://github.com/richzhang/PerceptualSimilarity\n",
        "    loss_fn=PerceptualLoss(net_type=\"squeeze\"), \n",
        "    learning_rate=LEARNING_RATE,\n",
        "    samples_path=SAMPLES_PATH,\n",
        "    save_samples_every_n_epoch=1,\n",
        "    num_samples=NUM_SAMPLES,\n",
        "    sample_steps=SAMPLE_STEPS,\n",
        "    sample_ema=True,\n",
        "    sample_seed=42,\n",
        ")"
      ],
      "metadata": {
        "id": "Hyr1B37TWPup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "You can see the generated images in `SAMPLES_PATH` or in **Wandb Workspace** as the training progresses."
      ],
      "metadata": {
        "id": "FRb1byQiZ7Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers.wandb import WandbLogger\n",
        "\n",
        "trainer = Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    logger=WandbLogger(project=\"consistency\", log_model=True),\n",
        "    callbacks=[\n",
        "        ModelCheckpoint(\n",
        "            dirpath=\"ckpt\", \n",
        "            save_top_k=3, \n",
        "            monitor=\"loss\",\n",
        "        )\n",
        "    ],\n",
        "    max_epochs=MAX_EPOCHS,\n",
        "    precision=16 if torch.cuda.is_available() else 32,\n",
        "    log_every_n_steps=30,\n",
        "    gradient_clip_algorithm=\"norm\",\n",
        "    gradient_clip_val=1.0,\n",
        ")\n",
        "\n",
        "trainer.fit(consistency, dataloader)"
      ],
      "metadata": {
        "id": "cpvh1C_BXO63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate samples \n",
        "\n",
        "You can now `sample` high quality images! ðŸŽ‰"
      ],
      "metadata": {
        "id": "37CcQylFcZYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consistency.sample(64, sample_steps=20)"
      ],
      "metadata": {
        "id": "KpJy-UhOYvsk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
